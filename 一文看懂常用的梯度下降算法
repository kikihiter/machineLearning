https://blog.csdn.net/u013709270/article/details/78667531

博客
学院
下载
图文课
TinyMind
论坛
APP
问答
商城
VIP会员
活动
招聘
ITeye
GitChat

搜博主文章
写博客
发Chat
传资源
登录注册
转
一文看懂常用的梯度下降算法
2017年11月29日 00:00:00 JeemyJohn 阅读数：50841


作者：叶    虎

编辑：祝鑫泉



640?wx_fmt=png&wxfrom=5&wx_lazy=1一

概述


梯度下降算法（Gradient Descent Optimization）是神经网络模型训练最常用的优化算法。对于深度学习模型，基本都是采用梯度下降算法来进行优化训练的。梯度下降算法背后的原理：目标函数640?wx_fmt=png&wxfrom=5&wx_lazy=1关于参数640?wx_fmt=png&wxfrom=5&wx_lazy=1的梯度将是目标函数上升最快的方向。对于最小化优化问题，只需要将参数沿着梯度相反的方向前进一个步长，就可以实现目标函数的下降。这个步长又称为学习速率640?wx_fmt=png&wxfrom=5&wx_lazy=1。参数更新公式如下：


640?wx_fmt=png&wxfrom=5&wx_lazy=1

其中640?wx_fmt=png&wxfrom=5&wx_lazy=1是参数的梯度，根据计算目标函数640?wx_fmt=png&wxfrom=5&wx_lazy=1采用数据量的不同，梯度下降算法又可以分为批量梯度下降算法（Batch Gradient Descent），随机梯度下降算法（Stochastic GradientDescent）和小批量梯度下降算法（Mini-batch Gradient Descent）。对于批量梯度下降算法，其640?wx_fmt=png&wxfrom=5&wx_lazy=1是在整个训练集上计算的，如果数据集比较大，可能会面临内存不足问题，而且其收敛速度一般比较慢。随机梯度下降算法是另外一个极端，640?wx_fmt=png&wxfrom=5&wx_lazy=1是针对训练集中的一个训练样本计算的，又称为在线学习，即得到了一个样本，就可以执行一次参数更新。所以其收敛速度会快一些，但是有可能出现目标函数值震荡现象，因为高频率的参数更新导致了高方差。小批量梯度下降算法是折中方案，选取训练集中一个小批量样本计算640?wx_fmt=png&wxfrom=5&wx_lazy=1，这样可以保证训练过程更稳定，而且采用批量训练方法也可以利用矩阵计算的优势。这是目前最常用的梯度下降算法。


对于神经网络模型，借助于BP算法可以高效地计算梯度，从而实施梯度下降算法。但梯度下降算法一个老大难的问题是：不能保证全局收敛。如果这个问题解决了，深度学习的世界会和谐很多。梯度下降算法针对凸优化问题原则上是可以收敛到全局最优的，因为此时只有唯一的局部最优点。而实际上深度学习模型是一个复杂的非线性结构，一般属于非凸问题，这意味着存在很多局部最优点（鞍点），采用梯度下降算法可能会陷入局部最优，这应该是最头疼的问题。这点和进化算法如遗传算法很类似，都无法保证收敛到全局最优。因此，我们注定在这个问题上成为“高级调参师”。可以看到，梯度下降算法中一个重要的参数是学习速率，适当的学习速率很重要：学习速率过小时收敛速度慢，而过大时导致训练震荡，而且可能会发散。理想的梯度下降算法要满足两点：收敛速度要快；能全局收敛。为了这个理想，出现了很多经典梯度下降算法的变种，下面将分别介绍它们。



01

Momentum optimization

冲量梯度下降算法是BorisPolyak在1964年提出的，其基于这样一个物理事实：将一个小球从山顶滚下，其初始速率很慢，但在加速度作用下速率很快增加，并最终由于阻力的存在达到一个稳定速率。对于冲量梯度下降算法，其更新方程如下：

0?wx_fmt=png

可以看到，参数更新时不仅考虑当前梯度值，而且加上了一个积累项（冲量），但多了一个超参0?wx_fmt=png，一般取接近1的值如0.9。相比原始梯度下降算法，冲量梯度下降算法有助于加速收敛。当梯度与冲量方向一致时，冲量项会增加，而相反时，冲量项减少，因此冲量梯度下降算法可以减少训练的震荡过程。TensorFlow中提供了这一优化器：tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=0.9)。



02 

NAG

NAG算法全称Nesterov Accelerated Gradient,是YuriiNesterov在1983年提出的对冲量梯度下降算法的改进版本，其速度更快。其变化之处在于计算“超前梯度”更新冲量项，具体公式如下：

0?wx_fmt=png

既然参数要沿着0?wx_fmt=png更新，不妨计算未来位置0?wx_fmt=png的梯度，然后合并两项作为最终的更新项，其具体效果如图1所示，可以看到一定的加速效果。在TensorFlow中，NAG优化器为：tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=0.9, use_nesterov=True)



0?wx_fmt=png

图1 NAG效果图



03

AdaGrad

AdaGrad是Duchi在2011年提出的一种学习速率自适应的梯度下降算法。在训练迭代过程，其学习速率是逐渐衰减的，经常更新的参数其学习速率衰减更快，这是一种自适应算法。其更新过程如下：


0?wx_fmt=png

其中是梯度平方的积累量，在进行参数更新时，学习速率要除以这个积累量的平方根，其中加上一个很小值是为了防止除0的出现。由于是该项逐渐增加的，那么学习速率是衰减的。考虑如图2所示的情况，目标函数在两个方向的坡度不一样，如果是原始的梯度下降算法，在接近坡底时收敛速度比较慢。而当采用AdaGrad，这种情况可以被改观。由于比较陡的方向梯度比较大，其学习速率将衰减得更快，这有利于参数沿着更接近坡底的方向移动，从而加速收敛。




0?wx_fmt=png

图2 AdaGrad效果图



前面说到AdaGrad其学习速率实际上是不断衰减的，这会导致一个很大的问题，就是训练后期学习速率很小，导致训练过早停止，因此在实际中AdaGrad一般不会被采用，下面的算法将改进这一致命缺陷。不过TensorFlow也提供了这一优化器：tf.train.AdagradOptimizer。





04

RMSprop

RMSprop是Hinton在他的课程上讲到的，其算是对Adagrad算法的改进，主要是解决学习速率过快衰减的问题。其实思路很简单，类似Momentum思想，引入一个超参数，在积累梯度平方项进行衰减：


0?wx_fmt=png

可以认为仅仅对距离时间较近的梯度进行积累，其中一般取值0.9，其实这样就是一个指数衰减的均值项，减少了出现的爆炸情况，因此有助于避免学习速率很快下降的问题。同时Hinton也建议学习速率设置为0.001。RMSprop是属于一种比较好的优化算法了，在TensorFlow中当然有其身影：tf.train.RMSPropOptimizer(learning_rate=learning_rate,momentum=0.9, decay=0.9, epsilon=1e-10)。

不得不说点题外话，同时期还有一个Adadelta算法，其也是Adagrad算法的改进，而且改进思路和RMSprop很像，但是其背后是基于一次梯度近似代替二次梯度的思想，感兴趣的可以看看相应的论文，这里不再赘述。



05

Adam
Adam全称Adaptive moment estimation，是Kingma等在2015年提出的一种新的优化算法，其结合了Momentum和RMSprop算法的思想。相比Momentum算法，其学习速率是自适应的，而相比RMSprop，其增加了冲量项。所以，Adam是两者的结合体：


0?wx_fmt=png

可以看到前两项和Momentum和RMSprop是非常一致的，由于和的初始值一般设置为0，在训练初期其可能较小，第三和第四项主要是为了放大它们。最后一项是参数更新。其中超参数的建议值是0?wx_fmt=png。Adm是性能非常好的算法，在TensorFlow其实现如下： tf.train.AdamOptimizer(learning_rate=0.001,beta1=0.9, beta2=0.999, epsilon=1e-08)。





二

学习速率


前面也说过学习速率的问题，对于梯度下降算法，这应该是一个最重要的超参数。如果学习速率设置得非常大，那么训练可能不会收敛，就直接发散了；如果设置的比较小，虽然可以收敛，但是训练时间可能无法接受；如果设置的稍微高一些，训练速度会很快，但是当接近最优点会发生震荡，甚至无法稳定。不同学习速率的选择影响可能非常大，如图3所示。

0?wx_fmt=png

图3 不同学习速率的训练效果








理想的学习速率是：刚开始设置较大，有很快的收敛速度，然后慢慢衰减，保证稳定到达最优点。所以，前面的很多算法都是学习速率自适应的。除此之外，还可以手动实现这样一个自适应过程，如实现学习速率指数式衰减：

0?wx_fmt=png


在TensorFlow中，你可以这样实现：


initial_learning_rate = 0.1
decay_steps = 10000
decay_rate = 1/10
global_step = tf.Variable(0, trainable=False)
learning_rate = tf.train.exponential_decay(initial_learning_rate,                           
                            global_step, decay_steps, decay_rate)
# decayed_learning_rate = learning_rate *
#                decay_rate ^ (global_step / decay_steps)
optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)
training_op = optimizer.minimize(loss, global_step=global_step)








三

总结


本文简单介绍了梯度下降算法的分类以及常用的改进算法，总结来看，优先选择学习速率自适应的算法如RMSprop和Adam算法，大部分情况下其效果是较好的。还有一定要特别注意学习速率的问题。其实还有很多方面会影响梯度下降算法，如梯度的消失与爆炸，这也是要额外注意的。最后不得不说，梯度下降算法目前无法保证全局收敛还将是一个持续性的数学难题。







四



参考文献



Anoverview of gradient descent optimization algorithms: http://sebastianruder.com/optimizing-gradient-descent/.

Hands-OnMachine Learning with Scikit-Learn and TensorFlow, Aurélien Géron, 2017.

NAG:http://proceedings.mlr.press/v28/sutskever13.pdf.

Adagrad:http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf.

RMSprop:http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf.

Adadelta:https://arxiv.org/pdf/1212.5701v1.pdf.

Adam:https://arxiv.org/pdf/1412.6980.pdf.

不同的算法的效果可视化：https://imgur.com/a/Hqolp.











欢迎大家加群在群中探讨

欢迎留言或赞赏。















推

荐

阅 

读

客官，来嘛，谷歌小菜请你尝尝！

趣谈深度学习核心----激活函数

朴素贝叶斯实战篇之新浪新闻分类

Object Detection R-CNN

史上最详细的XGBoost实战（上）






扫描个人微信号，

拉你进机器学习大牛群。

福利满满，名额已不多…

640.jpeg?




80%的AI从业者已关注我们微信公众号

0?wx_fmt=gif 0?wx_fmt=gif 0?wx_fmt=gif 0?wx_fmt=gif 0?wx_fmt=gif 0?wx_fmt=gif 0?wx_fmt=gif 0?wx_fmt=gif

0?wx_fmt=gif 0?wx_fmt=gif 0?wx_fmt=gif 0?wx_fmt=gif 0?wx_fmt=gif 0?wx_fmt=gif 0?wx_fmt=gif 0?wx_fmt=gif







阅读更多
想对作者说点什么？ 我来说一句
 qq_24059779
godli_one： 学到了，谢谢！(09-13 22:45#4楼)
bingfeiqiji
bingfeiqiji： 排版看着有点不舒服(06-13 18:49#3楼)
qq_26886681
qq_26886681： 文中符号感觉有点小错(06-09 11:10#2楼)查看回复(1)
DCX_abc
DCX_abc： 厉害！！！(12-06 12:33#1楼)
查看 5 条热评
梯度下降法的推导（非常详细、易懂的推导）
pengchengliu
 1841

梯度下降算法的公式非常简单，”沿着梯度的反方向（坡度最陡）“是我们日常经验得到的，其本质的原因到底是什么呢？为什么局部下降最快的方向就是梯度的负方向呢？也许很多朋友还不太清楚。没关系，接下来我将以通俗...

机器学习入门：线性回归及梯度下降
xiazdong
 11.4万

本文会讲到： (1)线性回归的定义 (2)单变量线性回归 (3)cost function：评价线性回归是否拟合训练集的方法 (4)梯度下降：解决线性回归的方法之一 (5)featu...

【图解】梯度下降
guomutian911
 3759

参考文章：http://blog.csdn.net/zhulf0804/article/details/52250220 如果读者对方向导数和梯度的定义不太了解，请先阅读上篇文章《方向导数与...

梯度（Gradient）与梯度下降法（Gradient Descent）
li_wen01
 1955

引言 　机器学习栏目记录我在学习Machine Learning过程的一些心得笔记，涵盖线性回归、逻辑回归、Softmax回归、神经网络和SVM等等，主要学习资料来自网上的免费课程和一些经典书籍，免...

机器学习入门系列04，Gradient Descent（梯度下降法）
zyq522376829
 2万

什么是梯度下降法？学习速率的引入；如何调整学习速率；Adagrad算法介绍；用泰勒展开式对梯度下降法进行数学理论支持...

梯度下降（Gradient Descent）小结
qq_34374664
 1018

前言：不得不感谢互联网的蓬勃发展，让知识与思维有充分的交流，让我有幸看到一些卓越的人他们的见解与思维总结，此文是学习BP神经网络（面向数学建模，比较浅显）中遇到梯度下降法，ML小白，因此查到此文，现在...

机器学习笔记2-梯度下降（Gradient decent）
Cracked_hitter
 749

转载请注明链接：http://blog.csdn.net/cracked_hitter/article/details/78442351 首先说明一下本系列文章的背景，笔者就读专业并非计算机相关专业...

浅谈对梯度下降法的理解
zhulf0804
 1.2万

浅谈梯度下降法   如果读者对方向导数和梯度的定义不太了解，请先阅读上篇文章《方向导数与梯度》。   前些时间接触了机器学习，发现梯度下降法是机器学习里比较基础又比较重要的一个求最小值的算法。梯度下降...

Logistic回归和梯度下降总结
XiaoXIANGZI222
 5005

原文：http://blog.csdn.net/dongtingzhizi/article/details/15962797  Logistic回归总结 PDF下载地址：http://down...

[机器学习] ML重要概念：梯度（Gradient）与梯度下降法（Gradient Descent）
walilk
 3.7万

本文介绍机器学习中重要的概念：梯度和梯度下降法，这是我们在学习MachineLearning算法时的核心概念之一，其实也就是我们在大学本科高等数学中的基础概念。...

相关热词 一文hmm 一文深度学习 一文看懂 一文回调函数 一文编码

JeemyJohn

 博客专家

关注
原创
115
粉丝
912
喜欢
502
评论
826
等级： 访问： 43万+ 积分： 6104 排名： 5839
勋章：
机器学习公众号

关注微信公众号，专

为机器学习入门者

最新文章
机器学习核心算法各个击破
LightGBM大战XGBoost，谁将夺得桂冠？
Vim实战技巧锦集
Spark资源调优
CNN模型之ShuffleNet
博主专栏

机器学习

阅读量：20688241 篇

Smile机器学习库实战

阅读量：35252 篇

Redis从原理到实战

阅读量：60353 篇

异常检测算法

阅读量：225172 篇

ElasticSearch从原理到实战

阅读量：237814 篇
个人分类
机器学习44篇
数据挖掘3篇
深度学习6篇
数据结构与算法6篇
大数据6篇
Spark7篇
TensorFlow4篇
C++8篇
Java14篇
Python6篇
密码学2篇
Scala2篇
Kafka2篇
Linux6篇
推荐算法4篇
GitHub1篇
展开

归档
2018年4月 2篇
2018年2月 1篇
2018年1月 3篇
2017年12月 24篇
2017年11月 7篇
2017年10月 7篇
2017年9月 3篇
2017年8月 5篇
2017年7月 7篇
2017年6月 7篇
2017年5月 5篇
2017年4月 7篇
2017年3月 4篇
2017年1月 11篇
2016年12月 27篇
2016年11月 11篇
2015年11月 1篇
展开

热门文章
一文看懂常用的梯度下降算法
阅读量：50501

AI大行其道，你准备好了吗？—谨送给徘徊于转行AI的程序员
阅读量：26469

机器学习中的数据不平衡解决方案大全
阅读量：25434

Linux上查看和停止所有java进程
阅读量：24822

TensorFlow官网访问不了
阅读量：23621

最新评论
Spark写ES的遇到的坑
weixin_39165515：你好，我想问下，有没有遇到过Elasticsearch安装了Shield验证的情况下，Spark读...

一文看懂常用的梯度下降算法
qq_24059779：学到了，谢谢！

一步步教你理解LSTM
weixin_35978398：厉害了，适合入门

无监督聚类算法该如何评价
qq_35550465：有用，不错

Isolation Forest算...
Lcharon：请问一下，通过峰度系数筛选有价值的特征，是峰度系数越高越有价值还是越低越有价值？

联系我们
客服
请扫描二维码联系客服
webmaster@csdn.net

400-660-0108

QQ客服 客服论坛

关于招聘广告服务 网站地图

©2018 CSDN版权所有 京ICP证09002463号

百度提供搜索支持

app
经营性网站备案信息

网络110报警服务

中国互联网举报中心

北京互联网违法和不良信息举报中心

CSDN APP
登录
注册


11

5



 
